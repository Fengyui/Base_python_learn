一、	理解定义：
不完全监督：只有一部分训练数据具备标签；
不确切监督：训练数据只具备粗粒度标签；
不准确监督：给出的标签并不总是真值；
完全监督：训练数据具备完整数据标签。

预测模型是从包含大量训练样本的训练数据集中学习，每个训练样本对应一个事件或对象

多示例学习：
如图像分类/检索/注释 ，文本分类 ，垃圾邮件检测 
医学诊断 ，面部/对象检测 ，对象类别发现 ，对象跟踪  等

k 近邻算法的基本思想
对于给定的待分类样本 x，计算它与训练样本集中所有样本的相似度，根据相似度找出 x 的 k 最相似的训练样本，然后根据这 k 个样本 的类别决定 x 所属的类别，将 k 个最相似的训练样本中最普遍的类标记作为预测值 赋给待分类的样本 x 。

one-hot用处： 就是将序号的影响也消除掉



二、	实例代码

一、	数据的读取处理
1.读取
file = r'C:\Users\dell\Desktop\holiday\CSVread\data\lung\lung_data_sample.csv'
data1 = pd.read_csv(file,index_col=None)
data2 = np.array(data1.values[:,0:16900],dtype=float)#只选有数的部分
label = np.array(data1.label.values)
2.处理
#将数据进行划分，对数据进行数据预处理
X_train, X_test, y_train, y_test = train_test_split(data2, label,test_size=1/5, random_state=0)






1.	使用os读取文件
import pandas as pd
import os
trainFile = r"C:\Users\dell\Desktop\CSVread\df_xjgs.csv"#r表示原始字符串
pwd = os.getcwd()
os.chdir(os.path.dirname(trainFile))
trainData = pd.read_csv(os.path.basename(trainFile),sep=',',encoding='gbk',engine='python')
os.chdir(pwd)


2.	读取csv文件
import csv
filename = "c:\df_xjgs.csv"
with open(filename) as f:
    reader = csv.reader(f)
    header_row = next(reader)
    print(header_row)


3.	orange中的domain
import random
import numpy as np
from Orange.data import Domain, Table
new_data = in_data.copy()

for inst in new_data:
  for f in inst.domain.attributes:
    inst[f] += random.gauss(0, 0.02)


4.	sklearn的运用
（1）预处理数据的方法：
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

X_scaled = preprocessing.scale(X_train)#预处理

X_scaled.mean(axis=0)#计算均值 对行进行操作
X_scaled.std(axis=0)#计算方差

#这是属于processing的一种方法，便于在训练集上和测试集上运用相同的方法进行缩放
scaler = preprocessing.StandardScaler().fit(X_train)
scaler.transform(X_train)
scaler.transform(X_test)

#将数据限制在一个范围之内，一般为0和1之间
From sklearn.processing import MinMaxScaler
min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)





(2)机器学习
from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
reg.coef_


#这是广义交叉验证
from sklearn import linear_model
reg = linear_model.RidgeCV(alphas=[0.1,1.0,10.0])
reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
reg.alpha_


#lasso(一个估计稀疏系数的线性模型)
 from sklearn import linear_model
 reg = linear_model.Lasso(alpha = 0.1)
 reg.fit([[0, 0], [1, 1]], [0, 1])
reg.predict([[1, 1]])


#使用lasso的基于L1的特征选择，实现降维
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
#X.shape
lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X)
X_new.shape


#一般在coef_中存放回归系数  shuffle函数试讲序列的所有元素随机排序


#随机梯度下降 
from sklearn.linear_model import SGDClassifier
X = [[0., 0.], [1., 1.]]
y = [0, 1]
clf = SGDClassifier(loss="hinge", penalty="l2")
clf.fit(X, y)
clf.predict([[2., 2.]])

clf = SGDClassifier(loss="log").fit(X, y)
clf.predict_proba([[1., 1.]])




三、xgboost
import numpy as np
import pandas as pd
from numpy import loadtxt 
from xgboost import XGBClassifier 
from xgboost import plot_importance 
from matplotlib import pyplot 
# load data and split data
file = r'C:\Users\dell\Desktop\.csv'
data1 = pd.read_csv(file,index_col=None)
data2 = np.array(data1.values[:,0:16900],dtype=float)#只选有数的部分
label = data1.label[:100]
#dataset = loadtxt(file)

#fit model no training data 
model= XGBClassifier()
model.fit(data2, label)

#plot feature importance 
plot_importance(model)###输出重要特征
pyplot.show()



基础知识：
rr.shape查看形状   data = data.T#进行转置
def eval_test():
l='[11,2,3,4,[5,6,7,8,9]]'
d="{'a':123,'b':456,'c':'789'}"
t='([1,3,5],[5,6,7,8,9],[123,456,789])'
print ('-------------')
print (type(l),type(eval(l)))
print(type(d),type(eval(d)))
print(type(t),type(eval(t)))
if __name__=="__main__":
eval_test()

输出结果
<class 'str'> <class 'list'>
<class 'str'> <class 'dict'>
<class 'str'> <class 'tuple'>

查看图片
import matplotlib.pyplot as plt

img1 = plt.imread(r"C:\Users\dell\Pictures\阳光鲜花\1.jpg")

plt.subplot(121)
plt.imshow(img1)

plt.show()

存储sample_data的方法
df = pd.read_csv(r"C:\Users\dell\Desktop\CSVread\data\
df_nmvf-mb-ns_mean_16900_zero.csv",index_col=None)
df1 = df.iloc[:100,:]
df1.to_csv(r"C:\Users\dell\Desktop\CSVread\data\lung\lung_data_sample")
df2 = pd.read_csv(r"C:\Users\dell\Desktop\CSVread\data\lung\
lung_data_sample",index_col=None)
data = np.array(df2.values[:,1:16901],dtype=float)#取出数的部分 还是原始数据


重命名
df2.rename(columns={0:16900},inplace = True)  #把列名修改,0，1,2,3，，这样就可以对应添加把两列连接起来了


导出数据：
import pandas as pd
file = r"C:\Users\dell\Desktop\daylife\CSVread\data_set\df_nmvf-mb-ns_mean_16900_zero.csv"
data1 = pd.read_csv(file,index_col=None)
data1 = data1[~data1['subclass'].isin([16])]
data1.to_csv(r"C:\Users\dell\Desktop\daylife\CSVread\data_set\df_new_lung_data.csv")

